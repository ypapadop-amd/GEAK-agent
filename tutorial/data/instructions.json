[
  {
    "instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton code provides an element-wise addition kernel named `add_kernel` and a corresponding wrapper function `add_wrapper`. The `add_kernel` is a decorated function with `@triton.jit`, enabling Just-In-Time compilation for optimized GPU execution. It receives pointers to two input tensors (`in_ptr0`, `in_ptr1`), an output tensor (`out_ptr`), the number of elements to process (`n_elements`), and a `BLOCK_SIZE`. Within the kernel, `pid` identifies the program's ID for the block dimension. The computation starts by calculating `block_start` as the base index for each block using `pid * BLOCK_SIZE`, and `offsets` as a range from `block_start` up to `BLOCK_SIZE`. A mask is applied using `offsets < n_elements` to prevent out-of-bound memory access, loading tensor values with `tl.load` for both input pointers. The values are summed and stored back in `out_ptr` using `tl.store`, still respecting the mask.\n\n            The `add_wrapper` function orchestrates the setup and execution of the kernel. It initializes the output tensor `out` using `torch.zeros_like(x)` for similar shape and dtype. The function determines the total number of elements using `x.numel()` and computes the number of blocks needed with `num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE`. Finally, `add_kernel[(num_blocks,)](x, y, out, n_elements, BLOCK_SIZE)` launches the kernel, passing the input and output pointers, number of elements, and block size. The final result is returned in `out`.\n            ",
    "input": "",
    "output": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    in_ptr0,\n    in_ptr1,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n\ndef add_wrapper(x, y):\n    out = torch.zeros_like(x)\n    \n    BLOCK_SIZE = 4\n    n_elements = x.numel()\n\n    # Calculate the number of blocks needed\n    num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    add_kernel[(num_blocks,)](x, y, out, n_elements, BLOCK_SIZE)\n\n    return out\n\n\n\n\n",
    "file": "add_example.py",
    "difficulty": "2"
  },
  {
    "instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel sin_kernel computes the element-wise sine of a 1D input array. It takes the following parameters: in_ptr0, a pointer to the input data; out_ptr, a pointer to the output data; n_elements, the total number of elements to process; and BLOCK_SIZE, a compile-time constant determining the block of data processed per program instance. The kernel uses tl.program_id(axis=0) to identify which block it is processing and calculates offsets within the data based on the block ID and BLOCK_SIZE. It loads input data using tl.load with masking to prevent out-of-bounds access. After applying tl.sin to compute the sine of each loaded element, it stores the result back into memory using tl.store, again using a mask to ensure only valid memory locations are written. The wrapper function sin_triton initializes the computation by determining the number of elements in the input tensor x, and invokes the sin_kernel, specifying the grid size as the number of elements and setting the BLOCK_SIZE to 4.\n            ",
    "input": "",
    "output": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef sin_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n\ndef sin_triton(x, out):\n    n_elements = x.numel()\n    sin_kernel[(n_elements,)](x, out, n_elements, BLOCK_SIZE=4)\n\n\n\n",
    "file": "sin_computation.py",
    "difficulty": "2"
  }
]