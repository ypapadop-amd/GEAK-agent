[
  {
    "instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton code provides an element-wise addition kernel named `add_kernel` and a corresponding wrapper function `add_wrapper`. The `add_kernel` is a decorated function with `@triton.jit`, enabling Just-In-Time compilation for optimized GPU execution. It receives pointers to two input tensors (`in_ptr0`, `in_ptr1`), an output tensor (`out_ptr`), the number of elements to process (`n_elements`), and a `BLOCK_SIZE`. Within the kernel, `pid` identifies the program's ID for the block dimension. The computation starts by calculating `block_start` as the base index for each block using `pid * BLOCK_SIZE`, and `offsets` as a range from `block_start` up to `BLOCK_SIZE`. A mask is applied using `offsets < n_elements` to prevent out-of-bound memory access, loading tensor values with `tl.load` for both input pointers. The values are summed and stored back in `out_ptr` using `tl.store`, still respecting the mask.\n\n            The `add_wrapper` function orchestrates the setup and execution of the kernel. It initializes the output tensor `out` using `torch.zeros_like(x)` for similar shape and dtype. The function determines the total number of elements using `x.numel()` and computes the number of blocks needed with `num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE`. Finally, `add_kernel[(num_blocks,)](x, y, out, n_elements, BLOCK_SIZE)` launches the kernel, passing the input and output pointers, number of elements, and block size. The final result is returned in `out`.\n            ",
    "input": "",
    "output": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    in_ptr0,\n    in_ptr1,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n\ndef add_wrapper(x, y):\n    out = torch.zeros_like(x)\n    \n    BLOCK_SIZE = 4\n    n_elements = x.numel()\n\n    # Calculate the number of blocks needed\n    num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    add_kernel[(num_blocks,)](x, y, out, n_elements, BLOCK_SIZE)\n\n    return out\n\n\n\n\n",
    "file": "add_example.py",
    "difficulty": "2"
  },
  {
    "instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel sin_kernel computes the element-wise sine of a 1D input array. It takes the following parameters: in_ptr0, a pointer to the input data; out_ptr, a pointer to the output data; n_elements, the total number of elements to process; and BLOCK_SIZE, a compile-time constant determining the block of data processed per program instance. The kernel uses tl.program_id(axis=0) to identify which block it is processing and calculates offsets within the data based on the block ID and BLOCK_SIZE. It loads input data using tl.load with masking to prevent out-of-bounds access. After applying tl.sin to compute the sine of each loaded element, it stores the result back into memory using tl.store, again using a mask to ensure only valid memory locations are written. The wrapper function sin_triton initializes the computation by determining the number of elements in the input tensor x, and invokes the sin_kernel, specifying the grid size as the number of elements and setting the BLOCK_SIZE to 4.\n            ",
    "input": "",
    "output": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef sin_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n\ndef sin_triton(x, out):\n    n_elements = x.numel()\n    sin_kernel[(n_elements,)](x, out, n_elements, BLOCK_SIZE=4)\n\n\n\n",
    "file": "sin_computation.py",
    "difficulty": "2"
  },
  {
    "instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton-accelerated function embedding_kernel is specialized for extracting and storing embedding vectors from a weight matrix for a sequence of token IDs. It uses program IDs to determine processing offsets and handles iteration over sequences with BLOCK_N and BLOCK_NN stride sizes. For each sequence, it computes token IDs and uses masks to ensure only valid data is loaded and processed. The weight matrix is addressed using a combination of token IDs and dimension offsets, facilitated by the stride of the weight tensor. The processed vectors are then stored into the 'out' tensor using calculated strides and masks, ensuring each output sequence position receives the correct embedding vector. The wrapping function, embedding, configures and invokes the kernel with appropriate grid settings, aligning the number of warps and stages for optimal performance.\n            ",
    "input": "",
    "output": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef embedding_kernel(\n    weight,\n    input_ids,\n    out,\n    vob_start_id,\n    vob_end_id,\n    stride_weight_seq,\n    stride_out_seq,\n    n_ctx,\n    hiden_size: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_NN: tl.constexpr,\n):\n    start_n = tl.program_id(0) * BLOCK_N\n\n    offs_nn = start_n + tl.arange(0, BLOCK_NN)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    for start_nn in range(0, BLOCK_N, BLOCK_NN):\n        start_nn = tl.multiple_of(start_nn, BLOCK_NN)\n        offs_seq = start_nn + offs_nn\n        n_ctx_mask = offs_seq < n_ctx\n        token_ids = tl.load(input_ids + offs_seq, mask=n_ctx_mask, other=vob_end_id)\n        id_mask = (token_ids >= vob_start_id) & (token_ids < vob_end_id)\n        token_ids = token_ids - vob_start_id\n        dim_mask = offs_d < hiden_size\n        load_mask = id_mask[:, None] & dim_mask[None, :]\n        store_mask = n_ctx_mask[:, None] & dim_mask[None, :]\n        vecs = tl.load(weight + token_ids[:, None] * stride_weight_seq + offs_d[None, :], mask=load_mask, other=0.0)\n        tl.store(out + offs_seq[:, None] * stride_out_seq + offs_d[None, :], vecs, mask=store_mask)\n\n@torch.no_grad()\ndef embedding(input_ids, weight: torch.Tensor, vob_start_id, vob_end_id, out: torch.Tensor):\n    BLOCK_N = 64\n    BLOCK_NN = 1\n    BLOCK_DMODEL = triton.next_power_of_2(weight.shape[1])\n    n_ctx = input_ids.shape[0]\n\n    grid = (triton.cdiv(n_ctx, BLOCK_N), 1, 1)\n\n    embedding_kernel[grid](\n        weight,\n        input_ids,\n        out,\n        vob_start_id,\n        vob_end_id,\n        weight.stride(0),\n        out.stride(0),\n        n_ctx=n_ctx,\n        hiden_size=weight.shape[1],\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_N=BLOCK_N,\n        BLOCK_NN=BLOCK_NN,\n        num_warps=1,\n        num_stages=1,\n    )\n\n\n\n",
    "file": "embedding_triton_kernel.py",
    "difficulty": "3"
  },
  {
    "instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel function `rmsnorm_triton` is designed to perform RMS normalization on a given 3D tensor `x`, which is stored in `x_ptr`. The normalization is applied over the last dimension of `x` with a specified size `N_SIZE`. The function takes pointers to the input tensor `x_ptr`, the RMS normalization weights `rms_w_ptr`, and the output tensor `output_ptr`. It also requires the strides for input tensor, weight, and output tensor, as well as the constants `N_SIZE`, `eps`, and `BLOCK_N_SIZE`.\n\n            Within the kernel, `pid_batch` and `pid_m` identify the current batch and row of the matrix being processed. The kernel calculates the variance by iterating over chunks of size `BLOCK_N_SIZE` and accumulating the sum of squares of the elements. This is divided by `N_SIZE` to get the variance. The reciprocal square root of the variance plus epsilon gives the `rstd`. The kernel then normalizes each element by multiplying it with `rstd` and the corresponding weight from `rms_w_ptr`, storing the result in `output_ptr`.\n\n            The wrapper function `rmsnorm_triton_wrapper` takes the input tensor `x`, the weight tensor `rms_w`, and an optional `eps` value. It extracts the batch size, M, and K dimensions from `x`, creates an output tensor of the same shape, and launches the kernel with the appropriate grid configuration.\n            ",
    "input": "",
    "output": "import torch\n\nimport triton\nimport triton.language as tl\n\n\ntorch.manual_seed(1234)\n\n\n@triton.jit\ndef rmsnorm_triton(x_ptr, rms_w_ptr, output_ptr,\n                   stride_x_batch, stride_x_m, stride_x_k,\n                   stride_rms_w,\n                   stride_out_batch, stride_out_m, stride_out_k,\n                   N_SIZE: tl.constexpr, eps: tl.constexpr, BLOCK_N_SIZE: tl.constexpr):\n    pid_batch = tl.program_id(0)\n    pid_m = tl.program_id(1)\n\n    offs_m = pid_batch * stride_x_batch + pid_m * stride_x_m\n    block_N = tl.arange(0, BLOCK_N_SIZE)\n    var = tl.zeros((BLOCK_N_SIZE,), tl.float32)\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        offs_n = block_n_start_idx + block_N\n        x_ptr_mask = offs_n < N_SIZE\n        x = tl.load(x_ptr + offs_m + offs_n * stride_x_k, mask=x_ptr_mask, other=0.0)\n        var += tl.extra.hip.libdevice.pow(x.to(tl.float32), 2)\n\n    var = tl.sum(var, axis=0) / N_SIZE\n    rstd = tl.math.rsqrt(var + eps)\n\n    # multiply by weight and add bias\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        offs_n = block_n_start_idx + block_N\n        x_ptr_mask = offs_n < N_SIZE\n        rms_w = tl.load(rms_w_ptr + offs_n * stride_rms_w, mask=x_ptr_mask)\n\n        x = tl.load(x_ptr + offs_m + offs_n * stride_x_k, mask=x_ptr_mask, other=0.0).to(tl.float32)\n        x_hat = x * rstd\n        out = x_hat * rms_w\n        out_off = pid_batch * stride_out_batch + pid_m * stride_out_m + offs_n * stride_out_k\n        tl.store(output_ptr + out_off, out, mask=x_ptr_mask)\n\n\ndef rmsnorm_triton_wrapper(x, rms_w, eps=1e-6):\n    batch, M, K = x.shape\n    assert rms_w.shape[-1] == K\n    out = torch.empty_like(x)\n    rmsnorm_triton[(batch, M,)](x, rms_w, out,\n                                *x.stride(),\n                                *rms_w.stride(),\n                                *out.stride(),\n                                N_SIZE=K, eps=eps, BLOCK_N_SIZE=1024,\n                                )\n    return out\n\n\n\n",
    "file": "rmsnorm.py",
    "difficulty": "2"
  }
]
