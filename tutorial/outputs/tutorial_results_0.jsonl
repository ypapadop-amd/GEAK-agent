{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton-accelerated function embedding_kernel is specialized for extracting and storing embedding vectors from a weight matrix for a sequence of token IDs. It uses program IDs to determine processing offsets and handles iteration over sequences with BLOCK_N and BLOCK_NN stride sizes. For each sequence, it computes token IDs and uses masks to ensure only valid data is loaded and processed. The weight matrix is addressed using a combination of token IDs and dimension offsets, facilitated by the stride of the weight tensor. The processed vectors are then stored into the 'out' tensor using calculated strides and masks, ensuring each output sequence position receives the correct embedding vector. The wrapping function, embedding, configures and invokes the kernel with appropriate grid settings, aligning the number of warps and stages for optimal performance.\n            ", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef embedding_kernel(\n    weight,\n    input_ids,\n    out,\n    vob_start_id,\n    vob_end_id,\n    stride_weight_seq,\n    stride_out_seq,\n    n_ctx,\n    hiden_size: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_NN: tl.constexpr,\n):\n    start_n = tl.program_id(0) * BLOCK_N\n\n    offs_nn = start_n + tl.arange(0, BLOCK_NN)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    for start_nn in range(0, BLOCK_N, BLOCK_NN):\n        start_nn = tl.multiple_of(start_nn, BLOCK_NN)\n        offs_seq = start_nn + offs_nn\n        n_ctx_mask = offs_seq < n_ctx\n        token_ids = tl.load(input_ids + offs_seq, mask=n_ctx_mask, other=vob_end_id)\n        id_mask = (token_ids >= vob_start_id) & (token_ids < vob_end_id)\n        token_ids = token_ids - vob_start_id\n        dim_mask = offs_d < hiden_size\n        load_mask = id_mask[:, None] & dim_mask[None, :]\n        store_mask = n_ctx_mask[:, None] & dim_mask[None, :]\n        vecs = tl.load(weight + token_ids[:, None] * stride_weight_seq + offs_d[None, :], mask=load_mask, other=0.0)\n        tl.store(out + offs_seq[:, None] * stride_out_seq + offs_d[None, :], vecs, mask=store_mask)\n\n@torch.no_grad()\ndef embedding(input_ids, weight: torch.Tensor, vob_start_id, vob_end_id, out: torch.Tensor):\n    BLOCK_N = 64\n    BLOCK_NN = 1\n    BLOCK_DMODEL = triton.next_power_of_2(weight.shape[1])\n    n_ctx = input_ids.shape[0]\n\n    grid = (triton.cdiv(n_ctx, BLOCK_N), 1, 1)\n\n    embedding_kernel[grid](\n        weight,\n        input_ids,\n        out,\n        vob_start_id,\n        vob_end_id,\n        weight.stride(0),\n        out.stride(0),\n        n_ctx=n_ctx,\n        hiden_size=weight.shape[1],\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_N=BLOCK_N,\n        BLOCK_NN=BLOCK_NN,\n        num_warps=1,\n        num_stages=1,\n    )\n\n\n\n", "filename": "embedding_triton_kernel.py", "test_code": "import torch\n\ndef test_embedding():\n    # \u53c2\u6570\u5b9a\u4e49\n    vocab_size = 1000         # \u8bcd\u6c47\u8868\u5927\u5c0f\n    embedding_dim = 512       # \u5d4c\u5165\u7ef4\u5ea6\n    sequence_length = 128     # \u8f93\u5165\u5e8f\u5217\u957f\u5ea6\n    vob_start_id = 10         # \u8bcd\u6c47\u8868\u8d77\u59cb ID\n    vob_end_id = 1000         # \u8bcd\u6c47\u8868\u7ed3\u675f ID\n\n    # \u521b\u5efa\u6d4b\u8bd5\u8f93\u5165\u5f20\u91cf\n    input_ids = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32, device='cuda'\n    )\n    weight = torch.randn(\n        vocab_size, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n    out = torch.zeros(\n        sequence_length, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n\n    # \u8c03\u7528\u5d4c\u5165\u51fd\u6570\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n\n    # \u4fdd\u5b58\u7ed3\u679c\n    results = {}\n    results['test_case_1'] = out.clone()\n\n    # \u6d4b\u8bd5\u4e0d\u540c\u7684\u8f93\u5165\n    input_ids = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32, device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n    results['test_case_2'] = out.clone()\n\n    # \u6d4b\u8bd5\u4e0d\u540c\u7684\u8bcd\u6c47\u8868\u8303\u56f4\n    vob_start_id = 0\n    vob_end_id = 500\n    input_ids = torch.randint(\n        vob_start_id, vob_end_id, (sequence_length,), dtype=torch.int32, device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n    results['test_case_3'] = out.clone()\n\n    # \u6d4b\u8bd5\u4e0d\u540c\u7684\u5d4c\u5165\u7ef4\u5ea6\n    embedding_dim = 256\n    weight = torch.randn(\n        vocab_size, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n    out = torch.zeros(\n        sequence_length, embedding_dim, dtype=torch.float32, device='cuda'\n    )\n    embedding(input_ids, weight, vob_start_id, vob_end_id, out)\n    results['test_case_4'] = out.clone()\n\n    return results\n\nresult_gold = test_embedding()", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef embedding_kernel(\n    weight,\n    input_ids,\n    out,\n    vob_start_id,\n    vob_end_id,\n    stride_weight_seq,\n    stride_out_seq,\n    n_ctx,\n    hiden_size: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_NN: tl.constexpr,\n):\n    # Get program IDs for parallel processing\n    pid_seq = tl.program_id(0)\n    pid_dim = tl.program_id(1)\n    \n    # Calculate sequence and dimension offsets\n    seq_start = pid_seq * BLOCK_N\n    dim_start = pid_dim * BLOCK_DMODEL\n    \n    # Create offset arrays\n    offsets_seq = tl.arange(0, BLOCK_N) + seq_start\n    offsets_dim = tl.arange(0, BLOCK_DMODEL) + dim_start\n    \n    # Iterate over sequences with BLOCK_NN stride\n    for seq_offset in range(0, BLOCK_NN, BLOCK_NN):\n        current_seq_offsets = offsets_seq + seq_offset\n        \n        # Create masks for valid sequence positions\n        seq_mask = current_seq_offsets < n_ctx\n        dim_mask = offsets_dim < hiden_size\n        \n        # Load token IDs with masking\n        token_ids = tl.load(input_ids + current_seq_offsets, mask=seq_mask, other=0)\n        \n        # Filter token IDs within vocabulary range\n        valid_token_mask = (token_ids >= vob_start_id) & (token_ids <= vob_end_id) & seq_mask\n        \n        # Calculate weight matrix addresses\n        weight_offsets = (token_ids[:, None] - vob_start_id) * stride_weight_seq + offsets_dim[None, :]\n        \n        # Create combined mask for weight loading\n        weight_mask = valid_token_mask[:, None] & dim_mask[None, :]\n        \n        # Load embedding vectors from weight matrix\n        embeddings = tl.load(weight + weight_offsets, mask=weight_mask, other=0.0)\n        \n        # Calculate output addresses\n        out_offsets = current_seq_offsets[:, None] * stride_out_seq + offsets_dim[None, :]\n        \n        # Create output mask\n        out_mask = seq_mask[:, None] & dim_mask[None, :]\n        \n        # Store embedding vectors to output tensor\n        tl.store(out + out_offsets, embeddings, mask=out_mask)\n\ndef embedding(input_ids, weight: torch.Tensor, vob_start_id, vob_end_id, out: torch.Tensor):\n    # Get tensor dimensions\n    n_ctx = input_ids.shape[0]\n    hiden_size = weight.shape[1]\n    \n    # Calculate strides\n    stride_weight_seq = weight.stride(0)\n    stride_out_seq = out.stride(0)\n    \n    # Define block sizes for optimal performance\n    BLOCK_DMODEL = 128\n    BLOCK_N = 32\n    BLOCK_NN = 32\n    \n    # Calculate grid dimensions\n    grid_seq = triton.cdiv(n_ctx, BLOCK_N)\n    grid_dim = triton.cdiv(hiden_size, BLOCK_DMODEL)\n    \n    # Configure kernel launch parameters\n    grid = (grid_seq, grid_dim)\n    \n    # Launch kernel with optimal settings for AMD GPU\n    embedding_kernel[grid](\n        weight,\n        input_ids,\n        out,\n        vob_start_id,\n        vob_end_id,\n        stride_weight_seq,\n        stride_out_seq,\n        n_ctx,\n        hiden_size=hiden_size,\n        BLOCK_DMODEL=BLOCK_DMODEL,\n        BLOCK_N=BLOCK_N,\n        BLOCK_NN=BLOCK_NN,\n        num_warps=4,\n        num_stages=2\n    )", "speedup": 4.2068}
