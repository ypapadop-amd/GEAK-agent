{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton code provides an element-wise addition kernel named `add_kernel` and a corresponding wrapper function `add_wrapper`. The `add_kernel` is a decorated function with `@triton.jit`, enabling Just-In-Time compilation for optimized GPU execution. It receives pointers to two input tensors (`in_ptr0`, `in_ptr1`), an output tensor (`out_ptr`), the number of elements to process (`n_elements`), and a `BLOCK_SIZE`. Within the kernel, `pid` identifies the program's ID for the block dimension. The computation starts by calculating `block_start` as the base index for each block using `pid * BLOCK_SIZE`, and `offsets` as a range from `block_start` up to `BLOCK_SIZE`. A mask is applied using `offsets < n_elements` to prevent out-of-bound memory access, loading tensor values with `tl.load` for both input pointers. The values are summed and stored back in `out_ptr` using `tl.store`, still respecting the mask.\n\n            The `add_wrapper` function orchestrates the setup and execution of the kernel. It initializes the output tensor `out` using `torch.zeros_like(x)` for similar shape and dtype. The function determines the total number of elements using `x.numel()` and computes the number of blocks needed with `num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE`. Finally, `add_kernel[(num_blocks,)](x, y, out, n_elements, BLOCK_SIZE)` launches the kernel, passing the input and output pointers, number of elements, and block size. The final result is returned in `out`.\n            ", "label": "\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    in_ptr0,\n    in_ptr1,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    y = tl.load(in_ptr1 + offsets, mask=mask)\n    output = x + y\n    tl.store(out_ptr + offsets, output, mask=mask)\n\ndef add_wrapper(x, y):\n    out = torch.zeros_like(x)\n    \n    BLOCK_SIZE = 4\n    n_elements = x.numel()\n\n    # Calculate the number of blocks needed\n    num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    # Launch the kernel\n    add_kernel[(num_blocks,)](x, y, out, n_elements, BLOCK_SIZE)\n\n    return out\n\n\n\n\n", "filename": "add_example.py", "test_code": "# Test the kernel with appropriate inputs\ndef test_add_kernel():\n    results = {}\n    \n    # Test case 1\n    x1 = torch.randn(16, device='cuda')\n    y1 = torch.randn(16, device='cuda')\n    out1 = add_wrapper(x1, y1)\n    results['test_case_1'] = out1\n\n    # Test case 2: Different size\n    x2 = torch.randn(8, device='cuda')\n    y2 = torch.randn(8, device='cuda')\n    out2 = add_wrapper(x2, y2)\n    results['test_case_2'] = out2\n\n    # Test case 3: Larger size\n    x3 = torch.randn(32, device='cuda')\n    y3 = torch.randn(32, device='cuda')\n    out3 = add_wrapper(x3, y3)\n    results['test_case_3'] = out3\n\n    # Test case 4: Edge case with zero elements\n    x4 = torch.randn(0, device='cuda')\n    y4 = torch.randn(0, device='cuda')\n    out4 = add_wrapper(x4, y4)\n    results['test_case_4'] = out4\n\n    return results\n\n# Run the test\nresult_gold = test_add_kernel()", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(\n    in_ptr0,\n    in_ptr1,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    val0 = tl.load(in_ptr0 + offsets, mask=mask)\n    val1 = tl.load(in_ptr1 + offsets, mask=mask)\n    result = val0 + val1\n    tl.store(out_ptr + offsets, result, mask=mask)\n\ndef add_wrapper(x: torch.Tensor, y: torch.Tensor):\n    assert x.shape == y.shape, \"Input tensors must have the same shape\"\n    out = torch.zeros_like(x)\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024  # Define block size for kernel execution\n    num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n\n    add_kernel[(num_blocks,)](\n        x,\n        y,\n        out,\n        n_elements,\n        BLOCK_SIZE\n    )\n    return out", "speedup": 1.0037}
{"instruction": "You are a expert in writing Triton operators for efficient GPU programming. Use triton language write a kernel and wrapper according following instruction.\n            The Triton kernel sin_kernel computes the element-wise sine of a 1D input array. It takes the following parameters: in_ptr0, a pointer to the input data; out_ptr, a pointer to the output data; n_elements, the total number of elements to process; and BLOCK_SIZE, a compile-time constant determining the block of data processed per program instance. The kernel uses tl.program_id(axis=0) to identify which block it is processing and calculates offsets within the data based on the block ID and BLOCK_SIZE. It loads input data using tl.load with masking to prevent out-of-bounds access. After applying tl.sin to compute the sine of each loaded element, it stores the result back into memory using tl.store, again using a mask to ensure only valid memory locations are written. The wrapper function sin_triton initializes the computation by determining the number of elements in the input tensor x, and invokes the sin_kernel, specifying the grid size as the number of elements and setting the BLOCK_SIZE to 4.\n            ", "label": "\nimport triton\nimport triton.language as tl\nimport torch\n\n@triton.jit\ndef sin_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: \"tl.constexpr\",\n):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(in_ptr0 + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(out_ptr + offsets, output, mask=mask)\n\ndef sin_triton(x, out):\n    n_elements = x.numel()\n    sin_kernel[(n_elements,)](x, out, n_elements, BLOCK_SIZE=4)\n\n\n\n", "filename": "sin_computation.py", "test_code": "def test_sin_triton():\n    results = {}\n    \n    # Test case 1\n    x1 = torch.tensor([0.0, 1.0, 2.0, 3.0], device='cuda')\n    out1 = torch.empty_like(x1)\n    sin_triton(x1, out1)\n    results['test_case_1'] = out1\n\n    # Test case 2\n    x2 = torch.tensor([4.0, 5.0, 6.0, 7.0], device='cuda')\n    out2 = torch.empty_like(x2)\n    sin_triton(x2, out2)\n    results['test_case_2'] = out2\n\n    # Test case 3\n    x3 = torch.tensor([8.0, 9.0, 10.0, 11.0], device='cuda')\n    out3 = torch.empty_like(x3)\n    sin_triton(x3, out3)\n    results['test_case_3'] = out3\n\n    # Test case 4\n    x4 = torch.tensor([12.0, 13.0, 14.0, 15.0], device='cuda')\n    out4 = torch.empty_like(x4)\n    sin_triton(x4, out4)\n    results['test_case_4'] = out4\n\n    return results\n\nresult_gold = test_sin_triton()", "predict": "import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef sin_kernel(\n    in_ptr0,\n    out_ptr,\n    n_elements,\n    BLOCK_SIZE: tl.constexpr,\n):\n    block_id = tl.program_id(axis=0)\n    offsets = block_id * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    \n    input_data = tl.load(in_ptr0 + offsets, mask=mask)\n    result = tl.sin(input_data)\n    tl.store(out_ptr + offsets, result, mask=mask)\n\ndef sin_triton(x: torch.Tensor, out: torch.Tensor):\n    assert x.is_cuda and out.is_cuda, \"Input and output tensors must be CUDA tensors.\"\n    n_elements = x.numel()\n    BLOCK_SIZE = 4\n\n    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n\n    sin_kernel[grid](\n        in_ptr0=x,\n        out_ptr=out,\n        n_elements=n_elements,\n        BLOCK_SIZE=BLOCK_SIZE\n    )", "speedup": 1.0001}
